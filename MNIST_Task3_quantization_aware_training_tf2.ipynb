{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_Task3_quantization_aware_training_tf2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNEFsevZTnNC4n3OY/ZMH2g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kathy-lee/Lena_MNIST_Network/blob/master/MNIST_Task3_quantization_aware_training_tf2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTTw9OZy3cV1",
        "colab_type": "code",
        "outputId": "5e270ad0-a6a4-4715-a002-0b39a5043204",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Network quantization in weights, in tensorflow2\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.layers import Dense, Conv2D, MaxPool2D, Flatten\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras import callbacks\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import TensorBoard\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "import seaborn as sns\n",
        "import scipy\n",
        "from scipy.optimize import curve_fit\n",
        "from scipy.stats import norm, t\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haFTRKMInYyQ",
        "colab_type": "code",
        "outputId": "711e0e04-2b00-4372-8a60-0608ecdc5a90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# load mnist dataset\n",
        "(trainX, trainY), (testX, testY) = mnist.load_data()\n",
        "\n",
        "# reshape dataset\n",
        "trainX = trainX.reshape((trainX.shape[0], 28, 28, 1))\n",
        "testX = testX.reshape((testX.shape[0], 28, 28, 1))\n",
        "# one hot encode target values\n",
        "train_Y = to_categorical(trainY)\n",
        "test_Y = to_categorical(testY)\n",
        "\n",
        "def preprocess_image(train, test):\n",
        "\t# convert from integers to floats\n",
        "\ttrain_norm = train.astype('float32')\n",
        "\ttest_norm = test.astype('float32')\n",
        "\t# normalize to range 0-1\n",
        "\ttrain_norm = train_norm / 255.0\n",
        "\ttest_norm = test_norm / 255.0\n",
        "\t\n",
        "\treturn train_norm, test_norm\n",
        "\t \n",
        "# scale dataset\n",
        "train_X, test_X = preprocess_image(trainX, testX)\n",
        "print('Raw train dataset size:')\n",
        "print(train_X.shape)\n",
        "print(train_Y.shape)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Raw train dataset size:\n",
            "(60000, 28, 28, 1)\n",
            "(60000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bhbjh-ebFWi",
        "colab_type": "code",
        "outputId": "7d652834-238e-433f-8206-ebdcc0ce6df9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "# split validation dataset from train dataset\n",
        "trainXX, ValX, trainYY, ValY = train_test_split(train_X, train_Y, test_size=0.1, random_state=42)\n",
        "print('Split out validation set:')\n",
        "print('New train dataset size:')\n",
        "print(trainXX.shape)\n",
        "print(trainYY.shape)\n",
        "print('Validation dataset size:')\n",
        "print(ValX.shape)\n",
        "print(ValY.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Split out validation set:\n",
            "New train dataset size:\n",
            "(54000, 28, 28, 1)\n",
            "(54000, 10)\n",
            "Validation dataset size:\n",
            "(6000, 28, 28, 1)\n",
            "(6000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAp_eg6rDVI-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create model with regularization\n",
        "def create_model_with_l2_regularization():\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(tf.keras.layers.Conv2D(6, [5, 5], input_shape=[28, 28, 1], activation='relu',\n",
        "                                  kernel_regularizer=tf.keras.regularizers.l2(0.001), name='conv_1'))\n",
        "  model.add(tf.keras.layers.MaxPool2D())\n",
        "  model.add(tf.keras.layers.Conv2D(16, [5, 5], activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001),name='conv_2'))\n",
        "  model.add(tf.keras.layers.MaxPool2D())\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  # add regularization to layers\n",
        "  model.add(tf.keras.layers.Dense(120, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001), use_bias = False, name='dense_1'))\n",
        "  model.add(tf.keras.layers.Dense(84, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001), use_bias = False, name='dense_2'))\n",
        "  model.add(tf.keras.layers.Dense(10, activation='softmax', kernel_regularizer=tf.keras.regularizers.l2(0.01), use_bias = False, name='dense_3'))\n",
        "  \n",
        "  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  \n",
        "  return model\n",
        "\n",
        "model = create_model_with_l2_regularization()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdF9344pkuOf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "b5cf52d3-a0da-4900-e096-7bbbe2c77677"
      },
      "source": [
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='./', histogram_freq=1,\n",
        "                         write_graph=True,\n",
        "                         write_images=True)\n",
        "# train model\n",
        "results = model.fit(trainXX,\n",
        "                    trainYY,\n",
        "                    epochs=5,\n",
        "                    batch_size=128,\n",
        "                    verbose=2,\n",
        "                    validation_data=(ValX, ValY),\n",
        "                    callbacks=[tensorboard_callback])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 54000 samples, validate on 6000 samples\n",
            "Epoch 1/5\n",
            "54000/54000 - 4s - loss: 0.7024 - accuracy: 0.8785 - val_loss: 0.3864 - val_accuracy: 0.9672\n",
            "Epoch 2/5\n",
            "54000/54000 - 3s - loss: 0.3460 - accuracy: 0.9663 - val_loss: 0.3055 - val_accuracy: 0.9713\n",
            "Epoch 3/5\n",
            "54000/54000 - 3s - loss: 0.2747 - accuracy: 0.9737 - val_loss: 0.2568 - val_accuracy: 0.9737\n",
            "Epoch 4/5\n",
            "54000/54000 - 3s - loss: 0.2342 - accuracy: 0.9766 - val_loss: 0.2141 - val_accuracy: 0.9783\n",
            "Epoch 5/5\n",
            "54000/54000 - 3s - loss: 0.2060 - accuracy: 0.9791 - val_loss: 0.1913 - val_accuracy: 0.9828\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOWZwgwAEGz5",
        "colab_type": "code",
        "outputId": "44ddcdef-8a5d-40ba-8aa9-61db3b455455",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# evaluate model\n",
        "t_start = time.clock()\n",
        "_,acc = model.evaluate(ValX, ValY, batch_size=128, verbose=2)\n",
        "t = time.clock() - t_start\n",
        "print(f\"Test Accuracy: {acc:.4f}, Inference time: {t:.2f}s\")\n",
        "\n",
        "model.save('old_model.h5')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6000/6000 - 0s - loss: 0.2113 - accuracy: 0.9758\n",
            "Test Accuracy: 0.9758, Inference time: 0.28s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3nYfocI9qZz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def from_float_to_Q(float_np_arr, m_bits, n_bits):\n",
        "   f = (1 << n_bits)\n",
        "   q_min = -(1 << (m_bits - 1))\n",
        "   q_max = (1 << (m_bits - 1))*1.0 - (1.0 / f)\n",
        "   q = np.round(float_np_arr * f) * (1.0 / f)\n",
        "   q[q > q_max] = q_max\n",
        "   q[q < q_min] = q_min\n",
        "   return q"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjUL8drDj7wH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# quantize the weights of dense layers using the same Q notation\n",
        "def quantize_model_with_Qnotation(old_model, N_fraction_bits, scale):\n",
        "  quantized_model = tf.keras.models.Sequential()\n",
        "  for no,layer in enumerate(old_model.layers):\n",
        "    weights = layer.get_weights()\n",
        "    if no == 7 or no == 6 or no == 5: # quantize only dense_2 and dense_3 layers\n",
        "      scale_weights = scale*weights[0]\n",
        "      quan_weights = from_float_to_Q(scale_weights,6-N_fraction_bits, N_fraction_bits)\n",
        "      quan_weights = quan_weights/scale\n",
        "      new_layer = tf.keras.layers.Dense(quan_weights.shape[1], activation='softmax',use_bias=False)\n",
        "      quantized_model.add(new_layer)\n",
        "      new_layer.set_weights([quan_weights])\n",
        "      print(f'quan_weights:{quan_weights[0,0:20]}')\n",
        "    else:\n",
        "      quantized_model.add(layer)\n",
        "      quantized_model.layers[no].set_weights(weights)\n",
        "      \n",
        "  return quantized_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdMrBcbvbiHz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NEW  quantize the weights of dense layers using the same Q notation\n",
        "def quantize_model_with_Qnotation_2(model, N_fraction_bits, scale):\n",
        "  quantized_model = tf.keras.models.Sequential()\n",
        "  for no,layer in enumerate(model.layers):\n",
        "    weights = layer.get_weights()\n",
        "    if no == 7 or no == 6 or no == 5: # quantize only dense_2 and dense_3 layers\n",
        "      scale_weights = scale*weights[0]\n",
        "      quan_weights = from_float_to_Q(scale_weights,6-N_fraction_bits, N_fraction_bits)\n",
        "      quan_weights = quan_weights/scale\n",
        "      #new_layer = tf.keras.layers.Dense(quan_weights.shape[1], activation='softmax',use_bias=False)\n",
        "      #quantized_model.add(new_layer)\n",
        "      #new_layer.set_weights([quan_weights])\n",
        "      print(f'quan_weights:{quan_weights[0,0:20]}')\n",
        "      model.layers[no].set_weights([quan_weights])\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uAW-dqp5rkr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def variance_fit(weights):\n",
        "  mu_esti, std_esti = norm.fit(weights)\n",
        "  #print(f'estimated mu/std: {mu_esti},{std_esti}')\n",
        "  Q_max = mu_esti + std_esti\n",
        "  Q_min = mu_esti - std_esti\n",
        "  #print(f'estimated variance interval: [{Q_min}, {Q_max}]')\n",
        "  return Q_min, Q_max\n",
        "\n",
        "def quantize_loss(weights, quantized_weights, scale):\n",
        "  w = quantized_weights/scale\n",
        "  loss = np.linalg.norm(weights-w)\n",
        "  loss = loss / np.linalg.norm(weights)\n",
        "\n",
        "  return loss\n",
        "\n",
        "def find_Qformat_and_scale(weights):\n",
        "  w = weights.flatten()\n",
        "  max_weights_abs = max(w)\n",
        "  #print(f'max/min of weights:{max(w)}, {min(w)}')\n",
        "  range_min, range_max = variance_fit(w)\n",
        "  range_weights_abs = max([range_max,range_min])\n",
        "  #print(f'variance of weights:{range_weights_abs}')\n",
        "\n",
        "  M = 10\n",
        "  loss_min = 100000\n",
        "  scale_opt = 0\n",
        "  N_bits_opt = 0\n",
        "  range_opt = 0\n",
        "\n",
        "  for r in np.linspace(range_weights_abs, max_weights_abs, M):\n",
        "    for k in range(0,6):\n",
        "      scale = (np.power(2,5-k) - 1.0/np.power(2,k))/r\n",
        "      #print(f'r:{r}, k:{k}')\n",
        "      s_weights = scale * weights\n",
        "      quantized_weights = from_float_to_Q(s_weights, 6-k, k)\n",
        "      loss = quantize_loss(weights, quantized_weights, scale)\n",
        "      #print(f'loss:{loss}\\n')\n",
        "      if loss < loss_min:\n",
        "        loss_min = loss\n",
        "        scale_opt = scale\n",
        "        N_bits_opt = k\n",
        "        range_opt = r\n",
        "\n",
        "  return N_bits_opt, scale_opt,range_opt\n",
        "\n",
        "# quantize the weights of dense layers using optimized Q notation\n",
        "def quantize_model(old_model):\n",
        "  quantized_model = tf.keras.models.Sequential()\n",
        "  for no,layer in enumerate(old_model.layers):\n",
        "    weights = layer.get_weights()\n",
        "    if no == 7 or no == 6 or no == 5: # quantize only dense_2 and dense_3 layers\n",
        "      print(f\"\\n{no} th layer:\")\n",
        "      N_fraction_bits, scale, range_r = find_Qformat_and_scale(weights[0])\n",
        "      print(f'optimal format: Q{6-N_fraction_bits}.{N_fraction_bits}, scale: {scale}, range:{range_r}')\n",
        "      scaled_weights = scale * weights[0]\n",
        "      quan_weights = from_float_to_Q(scaled_weights, 6-N_fraction_bits, N_fraction_bits)\n",
        "      quan_weights = quan_weights/scale\n",
        "      new_layer = tf.keras.layers.Dense(quan_weights.shape[1], activation='softmax',use_bias=False)\n",
        "      #new_layer.set_weights([quan_weights])\n",
        "      quantized_model.add(new_layer)\n",
        "      quantized_model.layers[no].set_weights([quan_weights])    \n",
        "    else:\n",
        "      quantized_model.add(layer)\n",
        "      quantized_model.layers[no].set_weights(weights)\n",
        "\n",
        "  return quantized_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0fY_Kyr529Y",
        "colab_type": "code",
        "outputId": "d1819261-f8d8-4893-ee8b-891fd592555a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "# quatize the model \n",
        "quantized_model_1 = quantize_model_with_Qnotation(model, 4, 10)\n",
        "quantized_model_2 = quantize_model_with_Qnotation_2(model, 4, 10)\n",
        "#quantized_model = quantize_model(model)\n",
        "quantized_model_1.save('quantized_model_1.h5')\n",
        "quantized_model_1.save('quantized_model_2.h5')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "quan_weights:[ 0.       0.      -0.00625  0.04375  0.0375   0.       0.04375  0.\n",
            "  0.01875  0.025   -0.025    0.08125  0.00625 -0.      -0.08125 -0.025\n",
            " -0.       0.0375  -0.      -0.025  ]\n",
            "quan_weights:[ 0. -0. -0. -0.  0.  0. -0.  0.  0. -0.  0.  0.  0.  0. -0. -0.  0.  0.\n",
            " -0. -0.]\n",
            "quan_weights:[-0.00625  0.06875 -0.01875 -0.01875 -0.01875  0.025   -0.00625 -0.01875\n",
            " -0.0125   0.     ]\n",
            "quan_weights:[ 0.       0.      -0.00625  0.04375  0.0375   0.       0.04375  0.\n",
            "  0.01875  0.025   -0.025    0.08125  0.00625 -0.      -0.08125 -0.025\n",
            " -0.       0.0375  -0.      -0.025  ]\n",
            "quan_weights:[ 0. -0. -0. -0.  0.  0. -0.  0.  0. -0.  0.  0.  0.  0. -0. -0.  0.  0.\n",
            " -0. -0.]\n",
            "quan_weights:[-0.00625  0.06875 -0.01875 -0.01875 -0.01875  0.025   -0.00625 -0.01875\n",
            " -0.0125   0.     ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7o2B2gReQo9",
        "colab_type": "code",
        "outputId": "3c8d5270-415b-4911-da44-8b276dc6d8b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# compile model\n",
        "quantized_model_1.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# inference\n",
        "t = time.clock()\n",
        "l,a = quantized_model_1.evaluate(ValX, ValY, batch_size=128, verbose=2)\n",
        "t = time.clock() - t\n",
        "print(f\"Test Accuracy: {a:.4f}, Inference time: {t:.2f}s\")\n",
        "\n",
        "# inference\n",
        "t = time.clock()\n",
        "l,a = quantized_model_2.evaluate(ValX, ValY, batch_size=128, verbose=2)\n",
        "t = time.clock() - t\n",
        "print(f\"Test Accuracy: {a:.4f}, Inference time: {t:.2f}s\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6000/6000 - 0s - loss: 2.3261 - accuracy: 0.1208\n",
            "Test Accuracy: 0.1208, Inference time: 0.39s\n",
            "6000/6000 - 0s - loss: 0.2119 - accuracy: 0.9755\n",
            "Test Accuracy: 0.9755, Inference time: 0.28s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1pQns3ZcX-k",
        "colab_type": "code",
        "outputId": "43f9ba37-4228-4867-be68-a91d08844b91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 813
        }
      },
      "source": [
        "# quantization aware training\n",
        "def quantization_aware_training(model, train_data, train_label, N_fraction_bits, iterations):\n",
        "\n",
        "  # split the dataset into mulitple batches\n",
        "  mini_batch = 128\n",
        "  num_of_batches = train_data.shape[0]//mini_batch\n",
        "  print(f're-split trainging data to {num_of_batches} batches: \\n')\n",
        "  trainX = train_data[0:mini_batch*num_of_batches,:,:,:]\n",
        "  trainY = train_label[0:mini_batch*num_of_batches,:]\n",
        "  print(trainX.shape, trainY.shape)\n",
        "  trainX_in_batches = tf.split(trainX, num_of_batches, axis=0)\n",
        "  trainY_in_batches = tf.split(trainY, num_of_batches, axis=0)\n",
        "  print(trainX_in_batches[0].shape)\n",
        "  print(trainY_in_batches[0].shape)\n",
        "  print('finish data split.')\n",
        "\n",
        "  print('quantization aware training:\\n')\n",
        "  for i in range(num_of_batches):\n",
        "    print(f'\\n{i}th mini batch:')\n",
        "    # quantize the weights of dense layers\n",
        "    for no,layer in enumerate(model.layers):\n",
        "      weights = layer.get_weights()\n",
        "      \n",
        "      if 'dense' in layer.name:\n",
        "        print(f'\\n{layer.name}:')\n",
        "        print(f'old weights:{weights[0][0,0:20]}')\n",
        "        #N_fraction_bits, scale, range_r = find_Qformat_and_scale(weights[0])\n",
        "        #print(f'optimal format: Q{6-N_fraction_bits}.{N_fraction_bits}, scale: {scale}, range:{range_r}')\n",
        "        scale=10\n",
        "        N_fraction_bits=4\n",
        "        scaled_weights = scale * weights[0]\n",
        "        quan_weights = from_float_to_Q(scaled_weights, 6-N_fraction_bits, N_fraction_bits)\n",
        "        quan_weights = quan_weights/scale\n",
        "        print(f'quantized weights:{quan_weights[0,0:20]}')\n",
        "        layer.set_weights([quan_weights])\n",
        "\n",
        "    # test directly on the quantized model\n",
        "    _,acc = model.evaluate(ValX, ValY, batch_size=128, verbose=2)\n",
        "    print(f\"Test Accuracy directly on quantized model: {acc:.4f}\")\n",
        "\n",
        "    # re-train the quantized model once with a mini_batch\n",
        "    model.fit(trainX_in_batches[i],trainY_in_batches[i], epochs=1, batch_size=mini_batch, verbose=2)\n",
        "\n",
        "  print('one epoch training finished.')\n",
        "  # evaluate model\n",
        "  t_start = time.clock()\n",
        "  _,acc = model.evaluate(ValX, ValY, batch_size=128, verbose=2)\n",
        "  t = time.clock() - t_start\n",
        "  print(f\"\\n\\nTest performance:\\n Test Accuracy: {acc:.4f}, Inference time: {t:.2f}s\")\n",
        "\n",
        "\n",
        "quantization_aware_training(model, trainXX, trainYY, 0, 1)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "re-split trainging data to 421 batches: \n",
            "\n",
            "(53888, 28, 28, 1) (53888, 10)\n",
            "(128, 28, 28, 1)\n",
            "(128, 10)\n",
            "finish data split.\n",
            "quantization aware training:\n",
            "\n",
            "\n",
            "0th mini batch:\n",
            "\n",
            "dense_1:\n",
            "old weights:[ 1.52355572e-22  4.36946973e-02 -2.50855312e-02  6.47369027e-03\n",
            "  1.24506401e-02  1.42243443e-04 -6.06684311e-21 -1.86290983e-02\n",
            "  1.85766313e-02  2.45001935e-10 -1.86811369e-02 -4.49444256e-28\n",
            " -2.50187106e-02  7.89597825e-13 -1.23875039e-02  0.00000000e+00\n",
            " -1.25505235e-02  6.05624914e-03  3.75601128e-02  6.39986806e-03]\n",
            "quantized weights:[ 0.       0.04375 -0.025    0.00625  0.0125   0.      -0.      -0.01875\n",
            "  0.01875  0.      -0.01875 -0.      -0.025    0.      -0.0125   0.\n",
            " -0.0125   0.00625  0.0375   0.00625]\n",
            "\n",
            "dense_2:\n",
            "old weights:[-8.1304867e-22  1.4298556e-22  4.4372022e-23 -3.2323978e-22\n",
            " -4.6382022e-22 -2.5403626e-22 -7.9085268e-23 -2.6236174e-23\n",
            " -8.1118586e-23 -2.6012392e-22  8.1014728e-23  6.2297747e-23\n",
            " -7.8635675e-23 -1.1101946e-22  3.5761226e-22 -2.0811329e-23\n",
            " -5.8001800e-23 -3.0925613e-23  2.1900464e-23  2.9336159e-22]\n",
            "quantized weights:[-0.  0.  0. -0. -0. -0. -0. -0. -0. -0.  0.  0. -0. -0.  0. -0. -0. -0.\n",
            "  0.  0.]\n",
            "\n",
            "dense_3:\n",
            "old weights:[ 0.05006552 -0.01873222  0.13119891 -0.01245984 -0.04989707  0.10609758\n",
            "  0.00608213  0.0500136  -0.09374078 -0.01868773]\n",
            "quantized weights:[ 0.05    -0.01875  0.13125 -0.0125  -0.05     0.10625  0.00625  0.05\n",
            " -0.09375 -0.01875]\n",
            "6000/6000 - 0s - loss: 0.1919 - accuracy: 0.9828\n",
            "Test Accuracy directly on quantized model: 0.9828\n",
            "Train on 128 samples\n",
            "128/128 - 0s - loss: 0.1518 - accuracy: 1.0000\n",
            "one epoch training finished.\n",
            "6000/6000 - 0s - loss: 0.1926 - accuracy: 0.9822\n",
            "\n",
            "\n",
            "Test performance:\n",
            " Test Accuracy: 0.9822, Inference time: 0.28s\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}